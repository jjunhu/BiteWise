{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "file_name = '../data/processed/summarized_insights.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Assuming your CSV has columns 'Combined Comments' for input and 'Summary' for output\n",
    "# Concatenate them with a special separator (e.g., \"<|summary|>\")\n",
    "df['training_example'] = df['Combined Comments'] + \" <|summary|> \" + df['Summary']\n",
    "\n",
    "# Optionally, shuffle the dataframe\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split the dataset (e.g., 80% train, 10% validation, 10% test)\n",
    "train, validate, test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                 [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "# Save to text files\n",
    "train['training_example'].to_csv('train.txt', index=False, header=False)\n",
    "validate['training_example'].to_csv('validate.txt', index=False, header=False)\n",
    "test['training_example'].to_csv('test.txt', index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanhu/anaconda3/envs/tf_metal/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955fa310f3be4b0b9bc2d63da2602b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1074 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb729b4d70fc4615a75661d84d9aa84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.100850820541382, 'eval_runtime': 2.7504, 'eval_samples_per_second': 70.899, 'eval_steps_per_second': 17.816, 'epoch': 1.0}\n",
      "{'loss': 3.2369, 'learning_rate': 2.672253258845438e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e037bc0bfd0743ee81de645deb179339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.058077096939087, 'eval_runtime': 3.0534, 'eval_samples_per_second': 63.864, 'eval_steps_per_second': 16.048, 'epoch': 2.0}\n",
      "{'loss': 2.8733, 'learning_rate': 3.445065176908752e-06, 'epoch': 2.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f50923cb54140efaea2ac1a16e14c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.056661367416382, 'eval_runtime': 2.7142, 'eval_samples_per_second': 71.844, 'eval_steps_per_second': 18.053, 'epoch': 3.0}\n",
      "{'train_runtime': 311.6287, 'train_samples_per_second': 13.776, 'train_steps_per_second': 3.446, 'train_loss': 3.0371584901152597, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Assuming you have 'train.txt', 'validation.txt', and 'test.txt' files with preprocessed text\n",
    "train_path = 'train.txt'\n",
    "validation_path = 'validate.txt'\n",
    "\n",
    "# Create text datasets\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=train_path,\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "validation_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=validation_path,\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "# Data collator used for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./gpt2_finetuned',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model('./gpt2_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2\n",
      "Full Text: \"Was a huge fan of their Pinto location on Christopher Street location, which had way more Thai focus and less fusion. This new spot really confuses me. As a loyal patron of the former now-closed location, I wanted to like it, and yet this is a place where somehow, everything had something off and wrong with it, and you sit there wondering what the heck went wrong. From the noodle dishes to the crab in a coconut, everything felt over-salted, under, not fresh. The best part was actually scraping the coconut out of the coconut shell - that was delicious! But I could have gotten that elsewhere. Disappointing :/\n",
      "Actual Summary: - The customer misses the Thai-focused menu and less fusion approach of the old Pinto location on Christopher Street.\n",
      "Predicted Summary: summarize: \"Was a huge fan of their Pinto location on Christopher Street location, which had way more Thai focus and less fusion. This new spot really confuses me. As a loyal patron of the former now-closed location, I wanted to like it, and yet this is a place where somehow, everything had something off and wrong with it, and you sit there wondering what the heck went wrong. From the noodle dishes to the crab in a coconut, everything felt over-salted, under, not fresh. The best part was actually scraping the coconut out of the coconut shell - that was delicious! But I could have gotten that elsewhere. Disappointing :/ <|summary|> Based on the customer reviews provided, here are some actionable insights for the restaurant:\n",
      "\n",
      "- Address the quality of the noodle dishes to ensure they meet customer expectations.\n",
      "- Investigate and rectify any issues with the noodle dishes to ensure they meet customer expectations.\n",
      "- Investigate and rectify any issues with the noodle dishes to ensure they meet customer expectations.\n",
      "- Investigate and rectify any issues with the noodle dishes to ensure they meet customer expectations.\n",
      "- Investigate and rectify any issues with the noodle dishes to ensure they meet customer expectations.\n",
      "- Investigate and rectify any issues with the noodle dishes to ensure they meet customer expectations.\n",
      "- Investigate and rectify any issues with the noodle dishes to ensure they meet customer expectations.\n",
      "- Investigate and rectify any issues with the noodle dishes to ensure they meet customer expectations.\n",
      "- Investigate and rectify any issues with the noodle dishes to ensure they meet customer expectations.\n",
      "- Investigate and rectify any issues with the noodle dishes to ensure they meet customer expectations.\n",
      "- Investigate and rectify any issues with the noodle dishes to ensure they meet customer expectations.\n",
      "- Investigate and rectify any issues with the noodle dishes to ensure they meet customer expectations.\n",
      "- Investigate and rectify any issues with the noodle dishes to ensure they meet customer expectations.\n",
      "- Investigate and rectify any issues with the noodle dishes to ensure they meet customer expectations.\n",
      "- Investigate and rectify any issues with the noodle dishes to ensure they meet customer expectations.\n",
      "- Investigate and rectify any issues with the noodle dishes to ensure they meet customer expectations.\n",
      "- Investigate and rectify any issues with the noodle dishes to ensure they meet\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = './gpt2_finetuned'  # Adjust the path to your fine-tuned model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# Function to generate summary\n",
    "def generate_summary(text, max_length=512):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\")\n",
    "    summary_ids = model.generate(inputs, max_length=max_length, num_beams=5, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Path to the test file\n",
    "test_file_path = 'test.txt'\n",
    "\n",
    "# Read a few examples from the test set\n",
    "num_examples = 3\n",
    "with open(test_file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "full_text, actual_summary = lines[0].split(\"<|summary|>\")\n",
    "predicted_summary = generate_summary(full_text.strip())\n",
    "\n",
    "print(f\"Example {i+1}\")\n",
    "print(\"Full Text:\", full_text.strip())\n",
    "print(\"Actual Summary:\", actual_summary.strip())\n",
    "print(\"Predicted Summary:\", predicted_summary)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
