{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Falconsai/text_summarization\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Falconsai/text_summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c193b39f8bac4cb088c35d8967c1c811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97593fc55ae84feb84509d868a7ce005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2db37db0b28416ca5597a157660b088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "file_name = '../data/processed/summarized_insights.csv'\n",
    "\n",
    "dataset = datasets.load_dataset(\"csv\", data_files=file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanhu/anaconda3/envs/tf_metal/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"Combined Comments\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"Summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(tokenized_data['input_ids'], tokenized_data['labels'], test_size=0.1)\n",
    "\n",
    "# Convert to TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_targets))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_inputs, val_targets))\n",
    "\n",
    "# Batch the datasets\n",
    "batch_size = 4\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compile() got an unexpected keyword argument 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jonathanhu/BiteWise/notebooks/t5.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jonathanhu/BiteWise/notebooks/t5.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jonathanhu/BiteWise/notebooks/t5.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Compile the model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jonathanhu/BiteWise/notebooks/t5.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39;49mcompile(optimizer\u001b[39m=\u001b[39;49moptimizer)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jonathanhu/BiteWise/notebooks/t5.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jonathanhu/BiteWise/notebooks/t5.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model\u001b[39m.\u001b[39mfit(train_dataset, validation_data\u001b[39m=\u001b[39mval_dataset, epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_metal/lib/python3.8/site-packages/torch/nn/modules/module.py:2545\u001b[0m, in \u001b[0;36mModule.compile\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2536\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompile\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   2537\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2538\u001b[0m \u001b[39m    Compile this Module's forward using :func:`torch.compile`.\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2543\u001b[0m \u001b[39m    See :func:`torch.compile` for details on the arguments for this function.\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2545\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcompile(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: compile() got an unexpected keyword argument 'optimizer'"
     ]
    }
   ],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
